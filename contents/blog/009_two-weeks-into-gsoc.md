---
title: Warming Up for The Coding Period
time: 2012/05/09 20:00
tags: gsoc, python, biopython, testing
---

It has been more than two weeks ago since GSoC announced its accepted students list. Officially, we have are given four weeks to bond with the community around our projects and get familiar with any supporting tools we will use (e.g. git and friends). Coding officialy starts after this four-week period.

For me specifically, I've spent the past two weeks getting to know the programs I'll use in [my project](http://google-melange.appspot.com/gsoc/project/google/gsoc2012/warindrarto/13001) better. There are several sequence search programs that I plan to support: [BLAST+](http://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastDocs&DOC_TYPE=Download), [HMMER](http://hmmer.janelia.org/), [BLAT](http://genome.ucsc.edu/goldenPath/help/blatSpec.html), Bill Pearson's [FASTA](http://fasta.bioch.virginia.edu/fasta_www2/fasta_down.shtml) tools, and [Water](http://emboss.open-bio.org/wiki/Appdoc:Water) plus similar programs from the EMBOSS suite (e.g. [Needle](http://emboss.open-bio.org/wiki/Appdoc:Needle) and [Stretcher](http://emboss.open-bio.org/wiki/Appdoc:Stretcher)). 
Although they all try to achieve a similar goal (finding sequences similar to your query), they do it using different algorithms. Consequently, the options they provide and the outputs they show are all different. Since one of my primary task is parsing their output files, it is important to have example files (test cases) of the programs' possible output variations. I'll then be able to test my code against these files, to ensure it's working properly.

The output variations depend mostly on the search itself (query vs database results) and the options set by the user. Since the search programs are command line programs, test case generation can be automated easily. So what I had to do boils down to finding the right sequences and setting the right options for each of these programs, that result in the test cases I want. I can then automate the process using some good ol' Python scripts.

To this end, Biopython makes the automation task even easier because it provides wrappers for some of the programs (BLAST+ and Water, specifically). These wrappers allow us to focus on the program options to set, without worrying about the command line syntax. For the other three programs that have not been wrapped, I decided to write my own wrappers. I figured that in addition to familiarizing myself with the programs further, writing the wrappers could also be a nice warm up before the coding period officially starts. I've put these wrappers ([BLAT](https://github.com/bow/gsoc/blob/master/blat/_BLAT.py), [FASTA](https://github.com/bow/gsoc/blob/master/fasta/_FASTA.py), and [HMMER](https://github.com/bow/gsoc/blob/master/hmmer/_HMMER.py)) up in my [gsoc repo](https://github.com/bow/gsoc), if anyone is interested in using them. The FASTA and BLAT wrappers are still somewhat incomplete (some options have not been wrapped yet), but they are usable enough if you're not using complex arguments.

As for the test case generators, they are also available in the gsoc repo. I suspect they could be useful when a new version of a search program is released. Using them, we can generate new test cases easily and test them to see if their parsers should be updated as well. One issue that I still have with these generators is perhaps the generation time. The BLAST+ test case generator requires considerably more time to finish its job (more than one hour for a complete run, while other generators finish in less than a minute). BLAST+ does have the largest number of test cases (121 for now), but this has more to do with the size of the databases I'm using. The current BLAST+ test case generator uses NCBI's refseq_rna and refseq_protein databases, each measuring several GBs in size. Using a small database will solve the problem, and this may be implemented in future iterations of the generators. I have already obtained the test cases I want for now, though, so this is not a high priority task at the moment.

Finally, if you are interested in taking a look at the test cases, they've also been uploaded to the same gsoc repo. They've proven their usefulness by helping me submit a [fix](https://github.com/biopython/biopython/pull/39) built upon a recent [bug report](https://redmine.open-bio.org/issues/3346) about handling BLAST+'s latest release (2.2.26) text output. Still, I realize that there may be some corner cases I've failed to cover so don't hesitate to notify me if you found one.

Having completed the test case generation, now I'll be focusing on learning more about XML parsing and SQLite, as they will be integral parts of my project. I'll need to better understand XML to handle BLAST+'s XML output, while SQLite will be used for implementing SearchIO indexing. Additionally, I'll continue completing the [SearchIO naming scheme](http://bit.ly/searchio-terms), to speed up coding later on. If you're interested in watching the actual SearchIO repo, I've also uploaded the skeleton code [here](https://github.com/bow/biopython/tree/searchio/Bio/SearchIO). It has yet to contain any useful code, but you can expect to see much more along the summer :).
